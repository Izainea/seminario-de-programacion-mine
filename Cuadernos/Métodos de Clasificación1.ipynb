{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: -5px;\n",
    "  text-align: center;\n",
    "  color: white;\n",
    "  font-size: 15px;\">\n",
    "   <img src=\"images/banner.jpg\" alt=\"MINE-Seminario de programación\" style=\"width:100%;\">\n",
    "  <h1 style=\"\n",
    "  position: absolute;\n",
    "  top: 5%;\n",
    "  left: 50%;\">Métodos de clasificación</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis discriminante\n",
    "\n",
    "Es un método de clasificación supervisada que discrimina según las características de entrada. En este apartado revisaremos la discriminación lineal y cuadrática que hace serparaciones creando superficies de decisión lineal y cuadrática, respectivamente. Un elemento interesante es el uso del terema de Bayes para determinar la probabilidad de que pertenezca a alguna de las clases.\n",
    "\n",
    "Cuando nos referimos al método del análisis discriminante lineal podemos pensar que es muy similar a la regresión logística, sin embargo, cuando la variable objetivo tiene más de dos categorias funciona mejor que RL, pues si las clases están muy separadas, los parámetros estimados en el modelo de regresión son inestables mientras que el análisis discriminante lineal (LDA, por sus siglas en inglés) no muestra complicaciones. \n",
    "\n",
    "Además, si el número de observaciones es bajo y la distribución de los predictores es aproximadamente normal en cada una de las clases, LDA es más estable que la regresión logística. No obstante si el análisis discriminante se aplica en un caso bivariado tendremos resultados similares.\n",
    "\n",
    "**Supuestos y restricciones**\n",
    "\n",
    "* LAs variables predictoras, de entrada o discriminantes (como quieran llamarlas) deben ser cuantitativas.\n",
    "* El número de variables discriminantes debe ser menor que el número de registros menos 2. $(p<n-2)$ \n",
    "* No colinealidad\n",
    "* Las matrices de covarianzas de cada grupo deben ser aproximadamente iguales (normalizar), si esto no se cumple puede seguir al cuadrático.\n",
    "* Las variables continuas deben serguir una distribución normal multivariante, (aunque hay referencias que muestran un buen resultado a pesar de no tener esta distribución).\n",
    "\n",
    "## Idea intuitiva del algoritmo (Tomado de [cienciadedatos.net](https://www.cienciadedatos.net/documentos/28_linear_discriminant_analysis_lda_y_quadratic_discriminant_analysis_qda#Idea_intuitiva))\n",
    "\n",
    "* Disponer de un conjunto de datos de entrenamiento (training data) en el que se conoce a que grupo pertenece cada observación.\n",
    "* Calcular las probabilidades previas (prior probabilities): la proporción esperada de observaciones que pertenecen a cada grupo.\n",
    "* Determinar si la varianza o matriz de covarianzas es homogénea en todos los grupos. De esto dependerá que se emplee LDA o QDA.\n",
    "* Estimar los parámetros necesarios para las funciones de probabilidad condicional, verificando que se cumplen las condiciones para hacerlo.\n",
    "* Calcular el resultado de la función discriminante. El resultado de esta determina a qué grupo se asigna cada observación.\n",
    "* Utilizar métodos de validación para estimar la eficiencia del clasificador.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planteamiento del problema Bayesiano de decisión (considerando observaciones a priori)\n",
    "\n",
    "* Como mencionamos antes, debemos conocer las  probabilidades a priori de que un individuo pertenezca a un grupo, estas probabilidades las notamos $\\pi_k$ y hace referencia a la probabilidad de que la observación pertenezca a $k$.\n",
    "\n",
    "* Requerimos la función de densidad de probabilidad condicional de $X$ para una observación que pertenece a $k$, $f_k(X)=P(X=x|Y=k)$. \n",
    "\n",
    "* La probabilidad a posteriori resulta ser la que nos interesa, indica cual es la probabilidad de que Y=k, dado que X=x. Es decir, $P(Y=k|X=x)$.\n",
    "\n",
    "...  y solito solo entra el teorema de Bayes ...\n",
    "\n",
    "$$P(Y=k|X=x)=\\cfrac{P(X=x|Y=k)\\cdot \\pi_k}{\\sum_{j=1}^{K}\\pi_jP(X=x|Y=j)}$$\n",
    " \n",
    "\n",
    "La clasificación con menor error (clasificación de Bayes) se consigue asignando la observación a aquel grupo que maximice la probabilidad a posteriori, es decir, se asignará cada observación a aquel grupo para el que $\\pi_k P(X=x|Y=k)$ sea mayor.\n",
    "\n",
    "### Discriminante lineal\n",
    "\n",
    "Observe que necesitamos la probabilidad poblacional de que una observación cualquiera pertenezca a cada clase $(\\pi_k)$ y la probabilidad poblacional de que una observación que pertenece a la clase $k$ adquiera el valor $x$ en el predictor, ($P(X=x|Y=k)$), usamos la muestra para estimar esta información:\n",
    "\n",
    "$$ \\pi_k=\\cfrac{n_k}{k},$$\n",
    "$$P(Y = k | X = x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} \\cdot exp(-\\frac{1}{2\\sigma^{2}_{k}}(x-\\mu_k)^2). $$\n",
    "\n",
    "Donde $n_x$ es la cantidad de elementos en la clase $k$  y $\\mu_k$ y $\\sigma_k$  son la media y la varianza para la clase $k$. Si la varianza es la misma en todos los datos tenemos:\n",
    "\n",
    "$$P( Y = k | X = x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot exp(-\\frac{1}{2\\sigma^{2}}(x-\\mu_k)^2)}{\\sum_{j=1}^K\\pi_j \\frac{1}{\\sqrt{2\\pi\\sigma}} \\cdot exp(-\\frac{1}{2\\sigma^{2}}(x-\\mu_j)^2)},$$\n",
    "\n",
    "finalmente aplicando logaritmo:\n",
    "$$log(P( Y = k | X = x) ) =  x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu^2_{k}}{2\\sigma^2} + log(\\pi_k).$$\n",
    "\n",
    "El término lineal en el nombre Análisis discriminante lineal se debe al hecho de que la función discriminatoria es lineal respecto de $x$.\n",
    "\n",
    "En la práctica, a pesar de tener una certeza considerable de que $X$ se distribuye de forma normal dentro de cada clase, los valores $\\mu_1, \\cdots,\\mu_k$, $\\pi_1,\\cdots,\\pi_k$ y $\\sigma_2$ se desconocen, por lo que tienen que ser estimados a partir de las observaciones. Las estimaciones empleadas en LDA son:\n",
    "\n",
    "$$\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i; y_i=k}x_i \\text{ y }$$\n",
    "$$\\hat{\\sigma}_k = \\frac{1}{N-K}\\sum_{k=1}^K\\sum_{i; y_i=k}(x_i-\\hat{\\mu}_k)^2.$$\n",
    "\n",
    "La clasificación de Bayes consiste en asignar cada observación $X = x$ a aquella clase para la que $P(Y=k|X=x)$ sea mayor.\n",
    "\n",
    "### Discriminante cuadrático (cada clase tiene su matriz de covarianza)\n",
    "\n",
    "El clasificador cuadrático (QDA por sus siglas en inglés) se asemeja en gran medida al LDA, con la única diferencia de que el QDA considera que cada clase k tiene su propia matriz de covarianza $(\\Sigma_k)$ y, como consecuencia, la función discriminante toma forma cuadrática:\n",
    "\n",
    "$$log(P( Y = k | X = x))= -\\frac{1}{2}\\text{log}|\\Sigma_k|-\\frac{1}{2}(x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k})+\\text{log}(\\pi_k)$$\n",
    "\n",
    "Para poder calcular la posterior probability a partir de esta ecuación discriminante es necesario estimar, para cada clase, $\\Sigma_k, \\mu_k$ y $\\pi_k$ a partir de la muestra. Cada nueva observación se clasifica en aquella clase para la que el valor de la probabilidad a posteriori sea mayor.\n",
    "\n",
    "QDA genera límites de decisión curvos por lo que puede aplicarse a situaciones en las que la separación entre grupos no es lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de LDA en Python (Tomado de [scikit-learn](https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html))\n",
    "\n",
    "Este ejemplo traza los elipsoides de covarianza de cada clase y límite de decisión aprendido por LDA y QDA. Los elipsoides muestran la desviación estándar doble para cada clase. Con LDA, la desviación estándar es la misma para todas las clases, mientras que cada clase tiene su propia desviación estándar con QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Colormap\n",
    "cmap = colors.LinearSegmentedColormap(\n",
    "    'red_blue_classes',\n",
    "    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n",
    "     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
    "     'white': [(0.2, 0.2, 0.2), (0,0,0)],\n",
    "     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\n",
    "plt.cm.register_cmap(cmap=cmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #############################################################################\n",
    "# Generate datasets\n",
    "def dataset_fixed_cov():\n",
    "    '''Generate 2 Gaussians samples with the same covariance matrix'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(2021)\n",
    "    C = np.array([[0., -0.23], [0.83, .23]])\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C) + np.array([1, 1]),\n",
    "             np.dot(np.random.randn(n, dim), C) + np.array([2, 0])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n), 2*np.ones(n)))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_cov():\n",
    "    '''Generate 2 Gaussians samples with different covariance matrices'''\n",
    "    n, dim = 300, 2\n",
    "    np.random.seed(2021)\n",
    "    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n",
    "    X = np.r_[np.dot(np.random.randn(n, dim), C),\n",
    "              np.dot(np.random.randn(n, dim), C.T) + np.array([5, 5])]\n",
    "    y = np.hstack((np.zeros(n), np.ones(n)))\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.dot(np.random.randn(300, 2), np.array([[0., -1.], [2.5, .7]]) * 2)\n",
    "B=np.dot(np.random.randn(300, 2), (np.array([[0., -1.], [2.5, .7]]) * 2).T)\n",
    "print(np.cov(A.T))\n",
    "print(np.cov(B.T))\n",
    "print('completo')\n",
    "Com=np.r_[A,B + np.array([1, 6])]\n",
    "y=np.hstack((np.zeros(300), np.ones(300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fixed_cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #############################################################################\n",
    "# Plot functions\n",
    "def plot_data(lda, X, y, y_pred, fig_index):\n",
    "    splot = plt.subplot(2, 2, fig_index)\n",
    "    if fig_index == 1:\n",
    "        plt.title('Linear Discriminant Analysis')\n",
    "        plt.ylabel('Data with\\n fixed covariance')\n",
    "    elif fig_index == 2:\n",
    "        plt.title('Quadratic Discriminant Analysis')\n",
    "    elif fig_index == 3:\n",
    "        plt.ylabel('Data with\\n varying covariances')\n",
    "\n",
    "    tp = (y == y_pred)  # True Positive\n",
    "    tp0, tp1, tp2 = tp[y == 0], tp[y == 1],tp[y==2]\n",
    "    X0, X1,X2 = X[y == 0], X[y == 1], X[y == 2]\n",
    "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
    "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
    "    X2_tp, X2_fp = X2[tp2], X2[~tp2]\n",
    "\n",
    "    # class 0: dots\n",
    "    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')\n",
    "    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',\n",
    "                s=20, color='orange')  # dark red\n",
    "\n",
    "    # class 1: dots\n",
    "    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')\n",
    "    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',\n",
    "                s=20, color='green')  # dark blue\n",
    "    \n",
    "    # class 2: dots\n",
    "    plt.scatter(X2_tp[:, 0], X2_tp[:, 1], marker='.', color='yellow')\n",
    "    plt.scatter(X2_fp[:, 0], X2_fp[:, 1], marker='x',\n",
    "                s=20, color='green')  # dark blue\n",
    "\n",
    "    # class 0 and 1 : areas\n",
    "    nx, ny = 200, 100\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n",
    "                         np.linspace(y_min, y_max, ny))\n",
    "    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z[:, 1].reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n",
    "                   norm=colors.Normalize(0., 1.), zorder=0,shading='auto')\n",
    "    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')\n",
    "    \n",
    "    # means\n",
    "    plt.plot(lda.means_[0][0], lda.means_[0][1],\n",
    "             '*', color='white', markersize=15, markeredgecolor='grey')\n",
    "    plt.plot(lda.means_[1][0], lda.means_[1][1],\n",
    "             '*', color='white', markersize=15, markeredgecolor='grey')\n",
    "\n",
    "    return splot\n",
    "\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, color):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 3 * v[0] ** 0.5, 3 * v[1] ** 0.5,\n",
    "                              180 + angle, facecolor=color,\n",
    "                              edgecolor='black', linewidth=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(0.2)\n",
    "    splot.add_artist(ell)\n",
    "    splot.set_xticks(())\n",
    "    splot.set_yticks(())\n",
    "    \n",
    "\n",
    "\n",
    "def plot_lda_cov(lda, splot):\n",
    "    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n",
    "    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n",
    "\n",
    "\n",
    "def plot_qda_cov(qda, splot):\n",
    "    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')\n",
    "    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')\n",
    "    \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8), facecolor='white')\n",
    "plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',\n",
    "             y=0.98, fontsize=15)\n",
    "for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n",
    "    # Linear Discriminant Analysis\n",
    "    lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "    y_pred = lda.fit(X, y).predict(X)\n",
    "    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n",
    "    plot_lda_cov(lda, splot)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Quadratic Discriminant Analysis\n",
    "    qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "    y_pred = qda.fit(X, y).predict(X)\n",
    "    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n",
    "    plot_qda_cov(qda, splot)\n",
    "    plt.axis('tight')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis discriminante lineal para reducción dimensional\n",
    "\n",
    "El conjunto de datos de Iris representa 3 tipos de flores de Iris (Setosa, Versicolour y Virginica) con 4 atributos: longitud del sépalo, ancho del sépalo, largo del pétalo y ancho del pétalo.\n",
    "\n",
    "El análisis de componentes principales (PCA) aplicado a estos datos identifica la combinación de atributos (componentes principales o direcciones en el espacio de características) que representan la mayor variación en los datos. Aquí graficamos las diferentes muestras en los 2 primeros componentes principales.\n",
    "\n",
    "El análisis discriminante lineal (LDA) intenta identificar los atributos que explican la mayor variación entre clases . En particular, LDA, a diferencia de PCA, es un método supervisado que utiliza etiquetas de clase conocidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of IRIS dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador ingenuo de Bayes\n",
    "\n",
    "Los métodos de clasificación de Bayes hacen referencia a una collección de métodos de clasificación supervisada basados en la aplicación del teorema de Bayes, en estos métosos se supone de manera ingenua que hay independencia condicional entre las diferentes características según la clase. Como vimos arriba, a partir del teorema de Bayes podemos afirmar que:\n",
    "\n",
    "$$ P (y \\mid x_1, \\dots, x_n) = \\frac {P (y) P (x_1, \\dots, x_n \\mid y)} {P (x_1, \\dots, x_n)}$$\n",
    "\n",
    "Si asumimos *'ingenuamente'* lo siguiente:\n",
    "\n",
    "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),$$\n",
    "\n",
    "tenemos:\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                                 \n",
    "Dado que $P(x_1,\\cdots,x_n)$ es constante dada la entrada, podemos usar la siguiente regla de clasificación:\n",
    "\n",
    "$$ \\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}$$\n",
    "\n",
    "y podemos usar la estimación máxima a posteriori (MAP) para determinar $P(y)$ y $P(x_i|y)$, $P(y)$ resulta ser una proporción y según como calculemos $P(x_i|y)$ tendremos los diferentes clasificadores de Bayes.\n",
    "\n",
    "ver [H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS.](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador ingenuo de Bayes Gaussiano\n",
    "\n",
    "En este caso \n",
    "\n",
    "$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$ \n",
    "\n",
    "$\\sigma_y$ y $\\mu_y$ son estimados a partir de máxima verosimilitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador ingenuo de Bayes Multinomial\n",
    "\n",
    "\n",
    "Implementa el algoritmo ingenuo de Bayes para datos distribuidos multinomialmente, y es una de las dos variantes clásicas de Bayes ingenuo que se utilizan en la clasificación de texto (donde los datos se representan normalmente como recuentos de vectores de palabras, aunque también se sabe que los vectores tf-idf funcionan bien en la práctica). \n",
    "\n",
    "$$P(x_i \\mid y)=\\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}$$\n",
    "\n",
    "donde $N_{yi} = \\sum_{x \\in T} x_i$ es el número de veces que aparece la característica $i$ en una muestra de la clase $y$ en el set de entrenamiento $T$ y $N_{y} = \\sum_{i=1}^{n} N_{yi}$.\n",
    "\n",
    "El parametro que se usan de suavisado es $\\alpha \\geq 0$ tiene en cuenta las características que no están presentes en las muestras de aprendizaje y evita probabilidades cero en cálculos posteriores. Fijando $\\alpha=1$ tenemos un suavizado de Laplace, mientras que para $\\alpha<1$ tenemos un suavizado de Lidstone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complemento del clasificador ingenuo de Bayes\n",
    "\n",
    "Implementa el algoritmo de complemento ingenuo de Bayes (CNB). CNB es una adaptación del algoritmo estándar multinomial ingenuo de Bayes (MNB) que es particularmente adecuado para conjuntos de datos desequilibrados. Específicamente, CNB usa estadísticas del complemento de cada clase para calcular los pesos del modelo. Los inventores de CNB muestran empíricamente que las estimaciones de parámetros para CNB son más estables que las de MNB. Además, CNB supera regularmente a MNB (a menudo por un margen considerable) en las tareas de clasificación de texto. El procedimiento para calcular los pesos es el siguiente:\n",
    "\n",
    "$$ \\begin{align}\\begin{aligned}\\hat{\\theta}_{ci} = \\frac{\\alpha_i + \\sum_{j:y_j \\neq c} d_{ij}}\n",
    "                         {\\alpha + \\sum_{j:y_j \\neq c} \\sum_{k} d_{kj}}\\\\w_{ci} = \\log \\hat{\\theta}_{ci}\\\\w_{ci} = \\frac{w_{ci}}{\\sum_{j} |w_{cj}|}\\end{aligned}\\end{align}$$ \n",
    "                         \n",
    "donde las sumas están sobre todos los documentos $j$  que no están en la clase $c$ , $d_{ij}$ es el recuento o el valor tf-idf del término $i$ en el documento $j$, $\\alpha_i$ es un hiperparámetro de suavizado como el que se encuentra en MNB, y $\\alpha = \\sum_{i} \\alpha_i$. La segunda normalización aborda la tendencia de los documentos más largos a dominar las estimaciones de parámetros en MNB. La regla de clasificación es:\n",
    "\n",
    "$$ \\hat{c} = \\arg\\min_c \\sum_{i} t_i w_{ci}$$\n",
    "\n",
    "es decir, se asigna un documento a la clase que es la coincidencia de complemento más pobre ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador ingenuo de Bayes Bernoulli\n",
    "\n",
    "Implementa los algoritmos ingenuos de clasificación y entrenamiento de Bayes para los datos que se distribuyen de acuerdo con distribuciones de Bernoulli multivariadas; es decir, puede haber varias características, pero se supone que cada una es una variable de valor binario (Bernoulli, booleano). Por lo tanto, esta clase requiere que las muestras se representen como vectores de características con valores binarios; si se le entrega cualquier otro tipo de datos, una BernoulliNBinstancia puede binarizar su entrada (dependiendo del parámetro binario).\n",
    "\n",
    "La regla de decisión para el ingenuo Bayes de Bernoulli se basa en\n",
    "\n",
    "$$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)$$\n",
    "\n",
    "que difiere de la regla multinomial de NB en que penaliza explícitamente la no ocurrencia de una característica $i$ eso es un indicador de la clase $y$, donde la variante multinomial simplemente ignoraría una característica que no ocurre.\n",
    "\n",
    "En el caso de la clasificación de texto, se pueden usar vectores de ocurrencia de palabras (en lugar de vectores de conteo de palabras) para entrenar y usar este clasificador. BernoulliNB podría funcionar mejor en algunos conjuntos de datos, especialmente aquellos con documentos más cortos. Es aconsejable evaluar ambos modelos, si el tiempo lo permite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador ingenuo de Bayes Categórico\n",
    "\n",
    "implementa el algoritmo de Bayes ingenuo categórico para datos distribuidos categóricamente. Asume que cada característica $i$, que es descrita por el índice, tiene su propia distribución categórica.\n",
    "\n",
    "Para cada característica $i$ en el set de entrenamiento $X$, CategoricalNB estima una distribución categórica para cada característica i de X condicionada a la clase y. El conjunto de índices de las muestras se define como $J = \\{ 1, \\dots, m \\}$ , con $m$ el número de muestras.\n",
    "\n",
    "\n",
    "La probabilidad de la categoría $t$ en característica $I$ de la clase dada $c$ se estima como:\n",
    "\n",
    "$$P(x_i = t \\mid y = c \\: ;\\, \\alpha) = \\frac{ N_{tic} + \\alpha}{N_{c} + \\alpha n_i},$$\n",
    "\n",
    "\n",
    "CategoricalNB asume que la matriz de la muestra $X$ está codificado (por ejemplo, con la ayuda de OrdinalEncoder) de modo que todas las categorías para cada característica están representados con números $0,1,\\cdots, n_i-1$ dónde $n_i$ es el número de categorías de funciones disponibles $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
