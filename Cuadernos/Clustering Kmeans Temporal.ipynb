{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e11149cd-a522-4316-adb8-54e4ef49a0d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"padding: -5px;\n",
    "  text-align: center;\n",
    "  color: white;\n",
    "  font-size: 15px;\">\n",
    "   <img src=\"images/banner.jpg\" alt=\"MINE-Seminario de programación\" style=\"width:100%;\">\n",
    "  <h1 style=\"\n",
    "  position: absolute;\n",
    "  top: 5%;\n",
    "  left: 50%;\">Clustering</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d1540e-7026-4e14-88d7-7243133051da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Agrupar datos o clustering hace referencia a una buena cantidad de técnicas no supervisadas usadas encontrar patrones o grupos (clusters) dentro de un conjunto de observaciones. La idea es particionar los datos de forma que, las observaciones que están dentro de un mismo grupo, son similares entre ellas y las observaciones de datos en diferentes grupos son distintos. \n",
    "\n",
    "Las aplicaciones del clustering en diferentes disciplinas (bioestadística, marketing…) permiten realizar tareas como segmentación de clientes o individuos, compresión de datos, entre otros. Hay una multitud de variantes y adaptaciones de sus métodos y algoritmos, sin embargo, pueden diferenciarse tres grupos principales:\n",
    "\n",
    " * **Partitioning Clustering**: Este tipo de algoritmos requieren que el usuario especifique de antemano el número de clusters que se van a crear (K-means, K-medoids, CLARA).\n",
    "\n",
    "* **Hierarchical Clustering**: Este tipo de algoritmos no requieren que el usuario especifique de antemano el número de clusters. (agglomerative clustering, divisive clusterig).\n",
    "\n",
    "* **Métodos que combinan o modifican los anteriores**: (hierarchical K-means, fuzzy clustering, model based clustering y density based clustering).\n",
    "\n",
    "La librería de python scikit-learn ofrece implementaciones eficientes de varias técnicas de agrupamiento. Esta figura muestra cómo distintos algoritmos se comportan con varios tipos de datos.\n",
    "\n",
    "[![img1](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png)](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "![tabla](https://raw.githubusercontent.com/madcentral/metodosestadisticos/master/images/clusters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7cbf11-c048-4b94-8156-260f99af546d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Distancia\n",
    "\n",
    "Todos los métodos de clustering tienen una cosa en común, para poder llevar a cabo las agrupaciones necesitan definir y cuantificar la similitud entre las observaciones. El término distancia se emplea dentro del contexto del clustering como cuantificación de la similitud o diferencia entre observaciones. Si se representan las observaciones en un espacio p dimensional, siendo p el número de variables asociadas a cada observación, cuando más se asemejen dos observaciones más próximas estarán, de ahí que se emplee el término distancia. La característica que hace del clustering un método adaptable a escenarios muy diversos es que puede emplear cualquier tipo de distancia, lo que permite al investigador escoger la más adecuada para el estudio en cuestión. A continuación, se describen algunas de las más utilizadas. \n",
    "\n",
    "* Distancia Euclidea\n",
    " $$ d_{euc}(p,q) = \\sqrt{(x_p - x_q)^2 + (y_p - y_q)^2}$$\n",
    " \n",
    "* Distancia de Manhatan\n",
    "$$d_{man}(p,q) = \\sum_{i=1}^n |(p_i - q_i)|$$\n",
    "\n",
    "* Correlación\n",
    "$$d_{cor}(p,q) = 1 - \\text{correlacion}(p,q) $$\n",
    "\n",
    "* Mahalanobis\n",
    "$$d_{mah}(p,q) = \\sqrt{(p-q)^T\\Sigma^{-1}(p-q)} $$\n",
    "\n",
    "\n",
    "Otras métricas en Python ver [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb7efbc-7d11-4598-b167-b035833e5cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## K-means\n",
    "\n",
    " K-Means agrupa los datos tratando de separar muestras en K grupos con la menor varianza interna posible. El valor K es especificado por el investigador. El objetivo es minimizar la suma de las varianzas internas de los diferentes clusters:\n",
    " \n",
    " **Algoritmo**\n",
    " \n",
    " 1. Asignar aleatoriamente un número entre 1 y K a cada observación. Esto sirve como asignación inicial aleatoria de las observaciones a los clusters.\n",
    "\n",
    "2. Iterar los siguientes pasos hasta que la asignación de las observaciones a los clusters no cambie o se alcance un número máximo de iteraciones establecido por el usuario.\n",
    "\n",
    "    1. Para cada uno de los clusters calcular su centroide. Entendiendo por centroide la posición definida por la media de cada una de las dimensiones (variables) de las observaciones que forman el cluster. Aunque no es siempre equivalente, puede entenderse como el centro de gravedad.\n",
    "\n",
    "    2. Asignar cada observación al cluster cuyo centroide está más próximo.\n",
    "    \n",
    "Este algoritmo garantiza que, en cada paso, se reduzca la intra-varianza total de los clusters hasta alcanzar un óptimo local. La siguiente imagen muestra cómo van cambiando las asignaciones de las observaciones a medida que se ejecuta cada paso del algoritmo.\n",
    "\n",
    "![kmeans](https://raw.githubusercontent.com/madcentral/metodosestadisticos/master/images/kmeans.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18997d09-3488-4df1-8bc4-555148158128",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Implementación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1b339b-9115-4ae1-8aa2-b685436fe500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Incorrect number of clusters\n",
    "model_kmeans = KMeans(n_clusters=2, random_state=random_state)\n",
    "y_pred=model_kmeans.fit_predict(X)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"Incorrect Number of Blobs\")\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
    "plt.title(\"Anisotropicly Distributed Blobs\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"Unequal Variance\")\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3,\n",
    "                random_state=random_state).fit_predict(X_filtered)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\n",
    "plt.title(\"Unevenly Sized Blobs\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clustering Kmeans Temporal",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
